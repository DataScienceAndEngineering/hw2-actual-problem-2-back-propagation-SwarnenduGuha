{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy4XxErfw06xTKPCIb4xMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScienceAndEngineering/hw2-actual-problem-2-back-propagation-SwarnenduGuha/blob/main/Copy_of_HW_2_Problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume that the input variables are augmented with a column of 1s."
      ],
      "metadata": {
        "id": "x1e8nb-DyoKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Add a column of 1s to the input features\n",
        "X = np.c_[X, np.ones(X.shape[0])]\n",
        "\n",
        "# Function to initialize weights for the neural network\n",
        "def initialize_weights(layers):\n",
        "weights = [np.random.randn(next_layer, prev_layer) for prev_layer, next_layer in zip(layers[:-1], layers[1:])]\n",
        "return weights\n",
        "\n",
        "# Function to perform the sigmoid activation\n",
        "def sigmoid(z):\n",
        "\"\"\"The sigmoid function.\"\"\"\n",
        "return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "class Network(object):\n",
        "def __init__(self, sizes):\n",
        "self.num_layers = len(sizes)\n",
        "self.sizes = sizes\n",
        "self.weights = initialize_weights(sizes)\n",
        "\n",
        "def feedforward(self, x):\n",
        "\"\"\"Return output of the network \"\"\"\n",
        "for w in self.weights:\n",
        "x = sigmoid(np.dot(w, x))\n",
        "return x\n",
        "\n",
        "def backprop(self, x, y):\n",
        "\"\"\"Return a tuple \"(nabla_w)\" representing the\n",
        "gradient for the cost function C_x. \"nabla_w\" is a list of numpy arrays\n",
        "representing the gradients of the weights.\"\"\"\n",
        "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "# feedforward\n",
        "activation = x\n",
        "activations = [x] # list to store all activations\n",
        "zs = [] # list to store all z vectors\n",
        "\n",
        "for w in self.weights:\n",
        "z = np.dot(w, activation)\n",
        "zs.append(z)\n",
        "activation = sigmoid(z)\n",
        "activations.append(activation)\n",
        "\n",
        "# backward pass\n",
        "delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "nabla_w[-1] = np.outer(delta, activations[-2])\n",
        "\n",
        "for l in range(2, self.num_layers):\n",
        "z = zs[-l]\n",
        "sp = sigmoid_prime(z)\n",
        "delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "nabla_w[-l] = np.outer(delta, activations[-l - 1])\n",
        "\n",
        "return nabla_w\n",
        "\n",
        "def cost_derivative(self, output_activations, y):\n",
        "\"\"\"Return the vector of partial derivatives \\partial C_x /\n",
        "\\partial a for the output activations.\"\"\"\n",
        "return (output_activations - y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the neural network with 4 input nodes (5 with bias), 3 hidden nodes, and 3 output nodes\n",
        "net = Network([5, 3, 3])\n",
        "\n",
        "# Example usage of backpropagation\n",
        "for i in range(len(X_train)):\n",
        "x = X_train[i]\n",
        "y = np.zeros(3)\n",
        "y[y_train[i]] = 1 # One-hot encode the target class\n",
        "nabla_w = net.backprop(x, y)\n",
        "# Update weights (you might want to add a learning rate here)\n",
        "for i in range(len(net.weights)):\n",
        "net.weights[i] -= learning_rate * nabla_w[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "D-KKmDdVyzCp",
        "outputId": "fc41ccba-002e-4426-8e3a-df3122e4a872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 14 (<ipython-input-1-5a142c86a69a>, line 15)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5a142c86a69a>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    weights = [np.random.randn(next_layer, prev_layer) for prev_layer, next_layer in zip(layers[:-1], layers[1:])]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code begins by preparing the iris dataset, introducing a bias term by appending a column of 1s to the input features, and subsequently partitioning the data into training and testing sets. The neural network is initialized with a specific architecture: 4 input nodes (including the bias), 3 hidden nodes, and 3 output nodes. The feedforward method is designed to calculate the network's output for a given input, traversing through the layers and applying the sigmoid activation function. For backpropagation, the backprop method is implemented to compute weight gradients utilizing a matrix-based approach. The outer product operation efficiently handles the weight update computations. The code then demonstrates the application of the neural network on the training data. During each iteration, it processes an input, one-hot encodes the target class, and employs backpropagation to iteratively update the weights. It iss emphasized that introducing a learning rate in the weight update process is essential for controlling the step size during optimization."
      ],
      "metadata": {
        "id": "Kso9PnRUzE_r"
      }
    }
  ]
}